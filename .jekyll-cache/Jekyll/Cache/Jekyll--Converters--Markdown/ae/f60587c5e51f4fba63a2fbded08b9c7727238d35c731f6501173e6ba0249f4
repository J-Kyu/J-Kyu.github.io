I"0x<h1 id="multi-layer-perceptron">Multi-layer Perceptron</h1>

<h2 id="objective">Objective</h2>

<ul>
  <li>ë³¸ ê³¼ì œì—ì„œëŠ” Multi-Layer Perceptronì„ êµ¬í˜„í•˜ì—¬ Two Moon Dataset ê³¼ Iris Dataset ë“¤ì„ í•™ìŠµí•˜ê³  ê²°ê³¼ë¥¼ ì‚°ì¶œí•œë‹¤. ì´ë¥¼ í†µí•´ C++ì˜ ë‹¤ì–‘í•œ Class ê¸°ë²•ì„ ì—°ìŠµí•œë‹¤</li>
</ul>

<h2 id="background">Background</h2>

<blockquote>
  <p>Perceptron, Gradient Descent, Multi-Layer Perceptron Error, backpropagation</p>
</blockquote>

<h3 id="perceptron">Perceptron</h3>

<ul>
  <li>
    <p>Percetronì€ Neural Networkì˜ ê°€ì¥ ê¸°ë³¸ ë‹¨ìœ„ì´ë‹¤.</p>
  </li>
  <li>
    <p>ìˆ˜ìƒëŒê¸°ë¥¼ í†µí•´ ë“¤ì–´ì˜¨ ë‹¤ìˆ˜ì˜ ì‹ í˜¸(\(x\))ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ <em>ë³€ì¡°(\(\text{activation } \sigma\))</em>í•˜ì—¬ ì¼ì • thresholdë¥¼ ë„˜ê¸°ëŠ” ê²½ìš°, ì‹ í˜¸ë¥¼ ì¶œë ¥(\(\hat{y}\))í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤.</p>
  </li>
  <li>
    <p>Perceptron: <strong>\(\hat{y} = \sigma(\Sigma{w_ix_i+b}) = \sigma(WX+b)\)</strong></p>

    <blockquote>
      <ul>
        <li>
          <p>ì…ë ¥(feature): \(X=(x_1,x_2,...,x_n)\)</p>
        </li>
        <li>
          <p>ì¶œë ¥ : \(\hat{y}\)</p>
        </li>
        <li>
          <p>Model Parameter</p>
          <ul>
            <li>ê°€ì¤‘ì¹˜: \(W = (w_1,w_2,...,w_n)^T\)</li>
            <li>bias: \(b\)</li>
          </ul>
        </li>
      </ul>

    </blockquote>
  </li>
  <li>
\[\sigma(x) = \frac{1}{1+e^{-x}}\]

    <blockquote>
      <p>sigmoid functionì˜ ê°’ì€ 0ì—ì„œ 0.5, ê·¸ë¦¬ê³  ìŒìˆ˜ì™€ ì–‘ìˆ˜ì¼ ë•Œ ê°ê° 0.5ë³´ë‹¤ ì‘ê³ 
0.5ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ë¯€ë¡œ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Trueì™€ Falseë¡œ ë‚˜ëˆ„ëŠ” activationì— ì í•©í•œ í•¨ìˆ˜ì´ë‹¤.</p>
    </blockquote>
  </li>
</ul>

<h3 id="gradient-descent">Gradient Descent</h3>

<blockquote>
  <p>Perceptronì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ Gradient Descent ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤.</p>
</blockquote>

<ul>
  <li>
    <p>Loss Function ëª©ì  í•¨ìˆ˜</p>

    <ul>
      <li>
        <p>Perceptionì˜ ê²°ê³¼ê°’ê³¼ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì •ë‹µê³¼ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì£¼ëŠ” í•¨ìˆ˜.</p>
      </li>
      <li>
        <p>ì •ë‹µê³¼ ê±°ë¦¬ê°€ ë©€ë©´ í° ê°’ì„, ê°€ê¹Œìš°ë©´ ì‘ì€ ê°’ì„ ì¶œë ¥í•˜ì—¬ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.</p>
      </li>
      <li>
        <p>ê°€ì¥ ê°„ë‹¨í•œ í•¨ìˆ˜ê°€ Mean Square Errorì´ë‹¤</p>

        <blockquote>
          <p><strong>\(\text{loss function}: L(y,\hat{y}) = \frac{(\hat{y}-y^2)}{2}\)</strong></p>
        </blockquote>

        <ul>
          <li><strong>yëŠ” Data ì‹¤ì œ ë ˆì´ë¸”(ì •ë‹µ</strong>). ì¦‰, ëª©ì  í•¨ìˆ˜ì˜ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì •ë‹µì— ê°€ê¹Œì›Œì§„ë‹¤.</li>
        </ul>
      </li>
      <li>
        <p>ì´ë•Œ, ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì˜ ê°’ì„ ìµœì†Œë¡œ í•˜ëŠ” <em>ìµœì ì˜ í•´</em>ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ <strong>Gradient</strong>ë¥¼ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆë‹¤.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Gradient Descent</p>

    <blockquote>
      <p>ê·¸ë¦¼ ì°¸ê³ </p>
    </blockquote>

    <ul>
      <li>ê·¸ë¦¼ì„ ë³´ë©´, GradientëŠ” íŠ¹ì • ì§€ì (ê·¸ë¦¼ì˜ initial weight)ì˜ tangential slopeì„ ë‚˜íƒ€ë‚¸ë‹¤</li>
      <li>ì´ë•Œ Model Parameterë¥¼ Gradient Negative ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•œë‹¤ë©´, ì´ê²ƒì„  loss function ê°’ì„ ìµœì†Œë¡œ ê°–ê²Œ í•˜ëŠ” ë°©ë²•ì´ë¼ëŠ” ê²ƒì„ ì§ì‘í•  ìˆ˜ ìˆë‹¤.</li>
      <li>ì´ëŸ° ê³¼ì •ì„ <strong>â€œí•™ìŠµâ€</strong>ì´ë¼ê³  í•œë‹¤
        <ul>
          <li>Model Parameterì„ Gradientê°€ Descentí•˜ê²Œ Updateí•˜ì—¬ loss functionì˜ ê°’ì´ ìµœì†Œë¡œ ê°–ê²Œ í•˜ëŠ” ê²ƒ</li>
        </ul>
      </li>
      <li>ì ê·¸ëŸ¼ ì´ì œ, gradient descentë¥¼ í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ parameterë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ êµ¬í•˜ì</li>
    </ul>
  </li>
</ul>

<h3 id="model-parameter">Model Parameter</h3>

<blockquote>
  <p>Model Parameter</p>

  <ul>
    <li>ê°€ì¤‘ì¹˜: \(W = (w_1,w_2,...,w_n)^T\)</li>
    <li>bias: \(b\)</li>
  </ul>
</blockquote>

<ul>
  <li>
    <p>Chain Ruleì„ í†µí•´ì„œ ê° Parameterì— ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê°’ì„ êµ¬í•œë‹¤</p>
  </li>
  <li>
    <p>\(a =\Sigma_iw_ix_i+b\)ë¼ê³  ì •ì˜ í• ë•Œ, <strong>Gradient</strong>ëŠ” ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ êµ¬í•œë‹¤</p>

    <blockquote>
      <ul>
        <li>
\[a =\Sigma_iw_ix_i+b\]
        </li>
        <li>L: loss function \(\text{loss function}: L(y,\hat{y}) = \frac{\hat{y}-y^2}{2}\)</li>
        <li>
\[\sigma(a) = \frac{1}{1+e^{-a}}\]
        </li>
        <li>\(\hat{y}\)=\(\sigma(XW+b)\)</li>
      </ul>
    </blockquote>
  </li>
</ul>

\[\frac{ğœ•L}{ğœ•\hat{y}}=\hat{y}-y\]

\[\frac{ğœ•L}{ğœ•a}=\sigma(a)(1-\sigma(a))\]

\[\frac{ğœ•a}{ğœ•w_i}=x_i,\frac{ğœ•a}{ğœ•b}=1\]

\[âˆ´\frac{ğœ•L}{ğœ•w_i}=\frac{ğœ•a}{ğœ•\hat{y}}\times \frac{ğœ•a}{ğœ•w_i}=(\hat{y}-y)\sigma(1-\sigma(a))x_i\]

\[âˆ´\frac{ğœ•L}{ğœ•b}=\frac{ğœ•L}{ğœ•\hat{y}}\times \frac{ğœ•\hat{y}}{ğœ•a}\times \frac{ğœ•a}{ğœ•b}=(\hat{y}-y)\sigma(1-\sigma(a))\]

<ul>
  <li>
    <p>ìœ„ì—ì„œ ê²°ê³¼ì ìœ¼ë¡œ ë‚˜ì˜¨ <strong>Gradient</strong> ì‹ì˜ ê°’ì´ ìµœì†Œê°’ì„ ê°–ë„ë¡í•˜ëŠ” <strong>Model Parameter</strong>ì„ êµ¬í•´ì•¼ í•œë‹¤.</p>
  </li>
  <li>
    <p>ë¬¸ì œëŠ”, ì´ëŒ€ë¡œ Gradient ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´, ìˆ˜ë ´í•˜ì§€ ì•Šê³  <strong>ì§„ë™</strong>ì„ í•œë‹¤</p>

    <blockquote>
      <p>ê³ ë¡œ, Learning Rate \(\eta\) ë¼ëŠ” <strong>hyper-parameter</strong>ì„ ì„¤ì •í•œë‹¤</p>
    </blockquote>
  </li>
</ul>

<h3 id="hyper-parameter">Hyper-Parameter</h3>

<ul>
  <li>
    <p>\(\eta\) ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´í•˜ëŠ”ë° ì˜¤ëœì‚¬ê°„ì´ ê±¸ë¦¬ê³ , ë„ˆë¬´ í¬ë©´ ë°œì‚°í•˜ê¸° ë•Œë¬¸ì— ì ë‹¹í•œ ê°’ì„ ê°€ì ¸ì•¼ í•œë‹¤</p>

    <blockquote>
      <p>Parameterê°€  updateí•˜ê¸° ìœ„í•´ì„œ  Old Valueì—ì„œ valueì˜ Gradientì„ Hyper-Parameterì™€ ê³±í•´ì„œ ëº€ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤</p>
    </blockquote>
  </li>
</ul>

\[w_i^{new} = w_i^{old}-\eta\frac{ğœ•L}{ğœ•w_i^{old}}\]

\[b^{new} = b^{old} - \eta\frac{ğœ•L}{ğœ•b^{old}}\]

<blockquote>
  <p>ì‚¬ì‹¤ paramterì„ êµ¬í•˜ê¸° ìœ„í•´ Gradient í•¨ìˆ˜ë¥¼ Chain Ruleì´ ì•„ë‹Œ ë” ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ, ë’¤ì— ë‚˜ì˜¬ <strong>Error Backpropagation</strong> ë•Œë¬¸ì´ë‹¤</p>
</blockquote>

<ul>
  <li>
    <p>ë” í¸í•œ ê³„ì‚°<sub>(Multi-Layerì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ë„ ìˆìŒ)</sub> ì„ ìœ„í•´ <strong>Vector</strong>ì„ ì‚¬ìš©í•˜ì—¬ ì¶•ì•½ëœ Gradientì‹ì„ ì‚¬ìš© í•  ê²ƒì´ë‹¤.(\(\nabla\)ëŠ” í¸ë¯¸ë¶„ ê¸°í˜¸ë¥¼ ëŒ€ì‹œ ì‚¬ìš©í•œë‹¤)</p>

    <blockquote>
      <p>í¸ë¯¸ë¶„??</p>
    </blockquote>
  </li>
</ul>

\[\nabla_{\hat{y}}L = \hat{y}-y\]

\[\nabla_{\hat{y}}L =\sigma(a)(1-\sigma(a))\]

\[\nabla_{w}a=X^T, \nabla_ba = 1\]

\[âˆ´\nabla_wL=\nabla_{\hat{y}}L\times\nabla_a\hat{y}\times\nabla_wa=(\hat{y}-y)\sigma(a)(1-\sigma(a))X^T\]

\[âˆ´\nabla_bL=\nabla_{\hat{y}}L\times\nabla_a\hat{y}\times\nabla_ba=(\hat{y}-y)\sigma(a)(1-\sigma(a))\]

<blockquote>
  <p>TëŠ” Matrixì˜ transposeì´ë‹¤â€¦.ê°€ë¡œë¥¼ ìƒˆë¡œë¡œ, ì„¸ë¡œë¥¼ ê°€ë¡œë¡œ</p>
</blockquote>

<ul>
  <li>ë‹¤ìŒ ì‹ì€ \(\nabla\)ë¥¼ ì ìš©í•œ hyper-paramterì´ë‹¤</li>
</ul>

\[w^{new} = w^{old}-\eta\nabla_{w^{old}}L\]

\[b^{new} = b^{old} - \eta\nabla_{b^{old}}L\]

<blockquote>
  <p>ì—¬ê¸°ê¹Œì§€ëŠ” ì˜¬ë°”ë¥¸ Hyper Parameterì„ êµ¬í•˜ê¸° ìœ„í•˜ì—¬ Gradientì„ êµ¬í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ í•˜ì˜€ë‹¤. ì´ì œ Perceptronì˜ ê°œë…ì´ í™•ì¥ëœ <strong>Multi-Layer Perceptron</strong>ì˜ ê°œë…ì— ëŒ€í•´ ì´ì•¼ê¸° í•˜ì</p>
</blockquote>

<h3 id="multi-layer-perceptron-1">Multi-Layer Perceptron</h3>

<blockquote>
  <ul>
    <li>ê°„ë‹¨í•˜ê²Œ, Multi-Layerì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ê·¸ êµ¬ì„±ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°ë¥¼ í•˜ì</li>
    <li>Muli-Layer Perceptionì€ ì—¬ëŸ¬ê°œì˜ Perceptionì„ stackingí•œ ê²ƒì„ ë§í•œë‹¤.  Stackingì„ í•˜ê¸° ë•Œë¬¸ì— ì¶œë ¥ê°’ì´ Scalarê°€ ì•„ë‹Œ Vector í˜•íƒœë¡œ ë‚˜ì™€ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤.</li>
    <li>ë˜í•œ, Perceptionì€ linear functionì´ê¸° ë–„ë¬¸ì—, nonlinearí•œ ì •ë³´ë¥¼ classifyí•˜ì—¬ì„œ Multi-Layer Perceptionì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•˜ë‹¤.</li>
  </ul>
</blockquote>

<ul>
  <li>
    <p>ê·¸ë¦¼ ì°¸ì¡°</p>

    <blockquote>
      <p>ê° ì›ì€ í•˜ë‚˜ì˜  perceptionì„ ì˜ë¯¸í•œë‹¤.</p>

      <ul>
        <li>Wì™€ bëŠ” parameterê°€ ëœê³ , Hiddent LayerëŠ” ê³„ì¸µì ì¸ ê´€ì ì—ì„œ Perceptionì˜ ì§‘í•©ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤</li>
      </ul>
    </blockquote>
  </li>
  <li>
    <p><strong>Hidden Layer 1</strong>ì˜ ìˆ˜ì‹ì´ë‹¤</p>

    <ul>
      <li>
        <p>\(m\)ì€ hidden layerì˜ perceptron ê°œìˆ˜ì´ë‹¤</p>

\[h_{1i}=\sigma(XW_{1i}+b_{1i}), i = \{1,....m\}\]

\[h_1=(h_{11},h_{_12},...h_{1m})\]
      </li>
    </ul>

    <blockquote>
      <p>\(h_n\)ì€ hidden layer nì„ ë‚˜íƒ€ë‚¸ë‹¤</p>
    </blockquote>
  </li>
  <li>
    <p>Hidden Layer 1ì˜ ì¶œë ¥ì€ Hidden Layer 2ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤. ë‹¤ìŒì‹ì€, <strong>Hidden Layer 2</strong>ì˜ ìˆ˜ì‹ì´ë‹¤</p>

    <ul>
      <li>
        <p>\(m\)ì€ ì—­ì‹ hidden layer 2ì˜ perceptron ê°œìˆ˜ì´ë‹¤</p>

\[h_{2j}=\sigma(h_1W_{2j}+b_{2j}),j=\{1,...j\}\]
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Matrix Multiplication</strong>ì„ ì‚¬ìš©í•˜ì—¬ í‘œí˜„í•˜ë©´ ë” ê°„ê²°í•˜ê²Œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤</p>
  </li>
  <li>
    <p><strong>Hidden Layer 1</strong></p>

    <blockquote>
      <ul>
        <li>\(W_{1nm}\)ì´ë‹¤, ì¦‰, ê°€ì¥ ì•ì˜ 1ì€ \(W_1\)ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ë‹¤.\(b\)ë„ ë™ì¼í•˜ë‹¤â€¦.Donâ€™t Confuse</li>
        <li>\(n\)ì€ input ê°’ì˜ <strong>ê°¯ìˆ˜ì´ë‹¤</strong></li>
      </ul>
    </blockquote>
  </li>
</ul>

\[W_1=\begin{bmatrix}
  W_{111} &amp; \cdots &amp; W_{1n1}\\
  \vdots &amp; \ddots &amp;\vdots \\
  W_{11m} &amp; \cdots &amp; W_{1nm}\\
  \end{bmatrix}
  =
  \begin{bmatrix}
  W_{11} &amp; \cdots &amp; W_{1m}
  \end{bmatrix}\]

\[b_1=\begin{bmatrix}
b_{11} &amp; \cdots &amp; b_{1m}
\end{bmatrix}\]

\[h_1 =\begin{bmatrix}
\sigma(XW_{11}+b_{11}) &amp; \cdots &amp; \sigma(XW_{1m}+b_{1m})
\end{bmatrix}
=\sigma(XW_{1}+b_{1})\]

<ul>
  <li><strong>Hidden Layer 2</strong>ë„ ê°™ì€ ë°©ë²•ìœ¼ë¡œ í‘œí˜„í•œë‹¤</li>
</ul>

\[\hat{y}=h_2=\sigma(h_1W_2+b_2)\]

\[\hat{y}=(\hat{y_1},\hat{y_2},...\hat{y_l})\]

<ul>
  <li>
    <p><strong>ë ˆì´ë¸” \(y\)</strong>ì˜ í‘œí˜„ ë²•</p>

    <ul>
      <li>ì•ì—ì„œ \(y\)ëŠ” dataì˜ ì‹¤ì œ ë ˆì´ë¸”(ì •ë‹µ)ì´ë¼ê³  í–ˆë‹¤</li>
      <li>ì´ \(y\)ë¥¼ í‘œí˜„ í• ë•Œ, <em>one-hot encoding</em>ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.</li>
      <li>\(y\)ê°€ ì´ 10ì˜ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ, 3ë²ˆ classì˜ lableì„ ê°€ì§„ë‹¤ê³  í•˜ë©´ \(y\)ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤</li>
    </ul>

\[y=(0,0,1,0,0,0,0,0,0,0)\]

    <blockquote>
      <p>í•´ì„ì„ í•˜ë©´, lableì„ ê°–ê³  ìˆì„ ìˆ˜ ìˆëŠ” \(y\)ì˜ 3ë²ˆì§¸ classì— lableì´ ìˆë‹¤ëŠ” ë§ì´ë‹¤??â€¦ê·¸ë˜ì„œ ë¹„êµ í•  ìˆ˜ ìˆë‹¤ëŠ” ë§??</p>
    </blockquote>

    <ul>
      <li>ìµœì¢… ê²°ê³¼ê°’ì¸ \(\hat{y}\)ì—ì„œ ê°€ì¥ ë†’ì€ ê°’ì„ ì¶”ì¶œí•˜ë©´ ê·¸ ê°’ê³¼  lableì„ ë¹„êµ í•  ìˆ˜ ìˆë‹¤.</li>
      <li>ë¹„êµí• ë•ŒëŠ”, ê°€ì¥ ë†’ì€ ê°’, argmaxì¸ \(\hat{y}\)ì™€ lableê³¼ ë¹„êµí•œë‹¤</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>lable \(y\)ê³¼ \(\hat{y}\)ì˜ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ì„œ</p>

  <ul>
    <li>ê·¸ë¦¼ 5ë¥¼ ì°¸ê³ í•˜ë¼. Inputì€ hidden layerì„ í†µí•´ì„œ outputì„ ë§Œë“¤ì–´ ë‚¸ë‹¤.
      <ul>
        <li>hidden layer ì•ˆì€ perceptronìœ¼ë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆê³ ,  input ê°’ê³¼ output ê°’ì˜ ê°œìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆê¸° ë–„ë¬¸ì—, hidden layer 1ì—ì„œëŠ” ë‹¤ìˆ˜ì˜ outputì„ ë§Œë“œëŠ”ë°, ì™œ hidden layer 2 ëŠ” í•˜ë‚˜ì˜  outputë§Œ ë§Œë“œëƒëŠ” ì§ˆë¬¸ì€ í•˜ì§€ ë§ì</li>
      </ul>
    </li>
    <li>ì´ë•Œ, hidden layerì„ í†µí•´ì„œ ë§Œë“  ê°’, ì¦‰, outputì´ \(\hat{y}\)ì´ê³ , \(y\)ëŠ”  lableë¡œì¨ ì •ë‹µì´ë‹¤.(ì£¼ì–´ì§„ ê°’ì¸ê°€??)</li>
    <li>ê³ ë¡œ, \(\hat{y}\)ì™€  \(y\)ëŠ” ê°™ì€ ì„±ì§ˆ(?)ì˜  outputì´ê¸° ë–„ë¬¸ì— ë¹„êµê°€ ê°€ëŠ¥í•˜ë‹¤</li>
  </ul>
</blockquote>

<ul>
  <li><strong>Classification</strong>
    <ul>
      <li>classificationì„ í†µí•´ì„œ \(\hat{y}\)ë¥¼ ì¶œë ¥í•œë‹¤ê³  í•œë‹¤.</li>
      <li>ì¦‰,  Hidden Layerë“¤ì„ ë§í•˜ëŠ” ê²ƒì´ë‹¤??</li>
    </ul>
  </li>
</ul>

<h3 id="error-backpropagation">Error Backpropagation</h3>

<blockquote>
  <ul>
    <li>ì ê¹ ì •ë¦¬ë¥¼ í•´ë³´ì</li>
    <li>ì§€ê¸ˆ ê¹Œì§€ëŠ” Parceptronì˜ ê°œë…ê³¼, ì˜¬ë°”ë¥¸ Parceptron, ì¦‰, model paramterë¥¼ êµ¬í•˜ê¸° ìœ„í•´ multil-layerì˜ ê°œë…ì„ ë°°ì› ë‹¤. Loss Functionrê³¼ Percentron ì‹ìœ¼ë¡œ ë¶€í„° Model Parameterì„ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë°°ì› ìœ¼ë‚˜, <strong>ì–´ë–»ê²Œ í•˜ë©´ <em>ì¢‹ì€</em> Model parameterë¥¼ êµ¬í•˜ëŠ”ì§€, ì¦‰ Updateë¥¼ í•˜ì—¬ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì•„ì§ ì•ˆë°°ì› ë‹¤.</strong></li>
    <li><em>ì¢‹ì€</em> Model Parameterì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ <strong>í•™ìŠµ ë°©ë²•</strong>ì´ë¼ê³  ì´ì•¼ê¸° í•  ìˆ˜ ìˆë‹¤. <strong>Error Backpropagation</strong>ì„ í†µí•´ì„œ í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ solutionì— ëŒ€í•´ ë°°ìš¸ ê²ƒ ì´ë‹¤.  (Gradient í•¨ìˆ˜ë¥¼ Chai Ruleë¡œ í‘¼ ì´ìœ )</li>
  </ul>
</blockquote>

<ul>
  <li>
    <p>Chain Ruleì„ ì‚¬ìš©í•˜ì—¬ ê° ê³„ì¸µì— ìˆëŠ” parameterì˜ Gradientë¥¼ êµ¬í•˜ì—¬ ë’¤ì—ì„œ ë¶€í„° <strong>ìˆœì°¨ì ìœ¼ë¡œ</strong> Updateí•˜ëŠ” ë°©ë²•ì´ë‹¤.</p>

    <ul>
      <li>Chain Ruleì„ í†µí•´ì„œ ë’¤ì—ì„œ ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ êµ¬í•˜ëŠ” ì´ìœ ëŠ”,  Multi-Layer Perceptionì˜ ì •ì˜ëŠ” ìœ„ì—ì„œ ë°°ìš´ ê²ƒ ê°™ì´ ëª…í™•í•˜ê²Œ ë‚´ë¦´ ìˆ˜ ìˆì§€ë§Œ, ê²Œì¸µì„ ìŒ“ê²Œ ë˜ë©´ Loss Functionì— ëŒ€í•œ ë§ˆë•…í•œ í•™ìŠµì„ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.</li>
      <li>ì¦‰, í•™ìŠµì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ì´ë‹¤</li>
    </ul>

    <blockquote>
      <p>Parameterì„ í†µí•´ Gradientë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.</p>

      <p>ê·¸ë¦¼ 5, ì²˜ëŸ¼ Hidden LayerëŠ” 2ê°œ ë¶„ì•„ë‹¤.(ë§ˆì§€ë§‰ Hidden LayerëŠ” 2ë²ˆì¨° ê²ƒ ì´ë‹¤)</p>
    </blockquote>

\[\text{Given: }a_2 = h_2W_2+b_2\]

\[L(y,\hat{y})=(\hat{y}-y)^2\]

\[\nabla_{\hat{y}}L=\hat{y}-y\]

\[\nabla_{a_2}\hat{y}=\sigma(a_2)(1-\sigma(a_2))\]

\[\nabla_{w_2}a_2=h_1^T,\nabla_{b_2}a_2=1=(1,....,1)^T\]

    <blockquote>
      <p>ì—¬ê¸°ì„œ 1ì€ vectorì´ë‹¤</p>
    </blockquote>

\[âˆ´\nabla_{w_2}L=\nabla_{\hat{y}}L\odot\nabla_{a_2}\hat{y}\times\nabla_{w_1}a_2=(\hat{y}-y)\odot\{\sigma(a_2)(1-\sigma(a_2))\}\times h_{1}^T\]

\[âˆ´\nabla_{b_2}L=\nabla_{\hat{y}L}\times \nabla_{b}a_2=(\hat{y}-y)\odot\{\sigma(a_2)\{1-\sigma(a_2\}\}\times 1\]

    <ul>
      <li>
        <p>ìœ„ì˜ ì‹ì‚¬ìš©í•˜ì—¬, <strong>Hidden Layer 2ì˜ Parameterì„ Update</strong> í•˜ì˜€ë‹¤
\(W_2^{new}=W_{2}^{old}-\eta\nabla_{w_2^{old}}L\)</p>

\[b_2^{new}=b_2^{old}-\eta\nabla_{b_2^{old}}L\]

        <blockquote>
          <ul>
            <li>
              <p>ìœ„ ì‹ì—ì„œëŠ” ì£¼ì–´ì§„ parameterë¥¼ Hidden Layer 2ì— ê¸°ì¤€ì—ì„œ ìƒˆë¡œìš´ parameterë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•˜ì˜€ë‹¤.</p>
            </li>
            <li>
              <p>Hyper-Parameterë¥¼ êµ¬í• ë•Œì™€ ê°™ì€ ë°©ë²•ì´ë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” Hidden Layer 2ì—ì„œ ë¶€í„° ê±°ê¾¸ë¡œ ê°€ê¸° ë•Œë¬¸ì—, ì´ì œ ìš°ë¦¬ëŠ” Hidden Layer 1ì— ëŒ€í•œ parameterì„ êµ¬í•  ìˆ˜ ìˆë‹¤</p>
            </li>
          </ul>
        </blockquote>
      </li>
      <li>
        <p><strong>Hidden Layer 1ì˜ Parameterì„ Update</strong> í•´ì£¼ì–´ì•¼ í•œë‹¤.</p>

        <ul>
          <li>Parameterì„ Updateí•˜ê¸° ìœ„í•´ì„œëŠ” \(h_1\)ì— ëŒ€í•œ Gradientë¥¼ chain ruleì„ ì‚¬ìš©í•˜ì—¬  \(\nabla_{w_1}{L}\)ì™€ \(\nabla_{b_1}L\)ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.</li>
          <li>\(a_2\)ì— ëŒ€í•œ Gradientê¹Œì§€ êµ¬í•œ ë¶€ë¶„ì— ì ìš© í•´ì•¼  \(\nabla_{w_1}{L}\)ì™€ \(\nabla_{b_1}L\)ì„ êµ¬í•  ìˆ˜ ìˆëŠ”ë°, ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ”  \(\nabla_{h_1}a_2\)ì„ ì•Œì•„ì•¼ í•œë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

\[\nabla_{h_1}a_2=W_{2}^T\]

\[\nabla_{h_1}L = \nabla_{\hat{y}}L\odot\nabla_{a_2}{\hat{y}} \times \nabla_{h_1}a_2=(\hat{y}-y)\odot\{\sigma(a_2)\{1-\sigma(a_2)\}\}\times W_2^T\]

<blockquote>
  <p>\(\nabla_{h_1}a_2\)ì„ êµ¬í–ˆìœ¼ë‹ˆ,  \(\nabla_{w_1}{L}\)ì™€ \(\nabla_{b_1}L\) êµ¬í•˜ì—¬ Hidden Layer 1ì˜ Parameterì„ Updateí•˜ì</p>
</blockquote>

\[\nabla_{a_1}h1=\sigma(a_1)(1-\sigma(a_1))\]

\[\nabla_{w_1}a_1=X^T, \nabla_{b_1}a_1= 1=(1,....,1)^T\]

\[\nabla_{W_1}L=\nabla_{h_1}L\odot\nabla_{a_1}h_1\times\nabla_{w_1}a_2\]

\[\nabla_{b_1}L=\nabla_{h_1}L\odot\nabla_{a_1}h_1\times\nabla_{b_1}a_2\]

\[W_{1}^{new}=W_{1}^{old}-\eta\nabla_{w_1^{old}}{L}\]

\[b_1^{new}=b_1^{old}-\eta\nabla_{b_1^{old}}L\]

<h3 id="multi-data">Multi-Data</h3>

<ul>
  <li>
    <p>ì§€ê¸ˆê¹Œì§€ëŠ” í•˜ë‚˜ì˜ Dataì— ëŒ€í•´ì„œë§Œ ì„¤ëª…ì„ í•˜ì˜€ì—ˆë‹¤. ë§Œì•½ <strong>ë‹¤ìˆ˜ì˜ data</strong>ì— ëŒ€í•œ í•™ìŠµì´ë¼ë©´, Gradientì˜ í•© ë˜ëŠ” í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ í•˜ë©´ëœë‹¤.  ë‹¤ìˆ˜ì˜ dataë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ í‘œê¸°í•œë‹¤</p>

    <ul>
      <li>ë§Œì•½, í•©ì„ ì‚¬ìš©í•œë‹¤ë©´, í‰ê· ì„ ì‚¬ìš©í• ë•Œì— ë¹„í•´ì„œ <strong>learning rate</strong>ì¡°ì •ì„ ì‹ ê²½ ì¨ì•¼ í•œë‹¤.</li>
    </ul>

\[X^i, i =\{1,2,....,N\}\]

    <ul>
      <li>ì´ í‘œí˜„ì—ì„œ \(i\)ëŠ” super scriptë¼ê³  í•˜ëŠ”ë°, \(i\)ë²ˆì¨° dataë¥¼ ëœ»í•œë‹¤</li>
    </ul>
  </li>
  <li>
    <p>super scriptë¥¼ ì ìš©í•˜ì—¬, <strong>ì†ì‹¤ í•¨ìˆ˜</strong>ë¥¼ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ëŠ”ë‹¤</p>

    <ul>
      <li>ì¶œë ¥ê³¼ ë ˆì´ë¸”(ì •ë‹µ)ì—ë„ ê°™ì€ Notationì„ ì ìš©í•œ ê²ƒì´ë‹¤</li>
    </ul>
  </li>
</ul>

\[L(y,\hat{y})=\Sigma_{i=1}^N(\hat{y}^i-y^i)^2\]

<h3 id="learning--data">Learning &amp; Data</h3>

<ul>
  <li>ë°ì´í„° í¬ê¸°ê°€ í° ì´ë¯¸ì§€ ë°ì´í„° ë“±ì€ ëª¨ë“  ë°ì´í„°ë¥¼ í•œë²ˆì— ì—…ë°ì´íŠ¸ í•˜ê¸°ê°€ í˜ë“¤ë‹¤.</li>
  <li>ê³ ë¡œ <strong>Mini Batch</strong>ë¼ëŠ” ê°œë…ì„ ì ìš©í•œë‹¤.
    <ul>
      <li>ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ iterativeí•˜ê²Œ í•™ìŠµí•˜ëŠ” ê²ƒ.</li>
    </ul>
  </li>
  <li>ëª¨ë¸ì„ í•™ìŠµí• ë•Œ,  <strong>epoch</strong>ëŒë©´ì„œ í•œë‹¤
    <ul>
      <li>í•œ ë°ì´í„° ì„¸ì„ ì „ë¶€ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” iterationì„ ëœ» í•œë‹¤</li>
    </ul>
  </li>
  <li>í•™ìŠµ ì‹œ, DataëŠ” <strong>Train,validation, test</strong>ë¡œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•œë‹¤
    <ul>
      <li><strong>Train Data</strong> ì€ í•™ìŠµì„ ìœ„í•´ ì‚¬ìš©ëœë‹¤.</li>
      <li><strong>Validation Data</strong>ì€ í•™ìŠµ ê³¼ì •ì„ ê²€ì¦í•˜ëŠ”ë°, ì‚¬ìš©ë˜ëŠ”ë°, í•™ìŠµì— ì‚¬ìš©ì´ ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•˜ë‹¤.</li>
      <li><strong>Test Data</strong> ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¡œ ë³´í†µ ë ˆì´ë¸”ì„ ê³µê°œí•˜ì§€ ì•Šê³  ê³µì •í•œ ëª¨ë¸ í‰ê°€ì— ì‚¬ìš©ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="program-explaination">Program Explaination</h2>

<h3 id="goal">Goal</h3>

<ul>
  <li>ê·¸ë¦¼ 6ì€, <strong>Two Moon Dataset</strong>ì´ë‹¤.
    <ul>
      <li>ì´ ê·¸ë¦¼ì—ì„œëŠ” non-linearí•œ 2-feature(2ê°œì˜ input), 2-class(2ê°œì˜ output) datasetì„ ë³¼ ìˆ˜ ìˆë‹¤.</li>
    </ul>
  </li>
  <li>ê·¸ë¦¼ì— ì£¼ì–´ì§„ dataë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” <strong>classifier</strong>ë¥¼ í•™ìŠµì‹œí‚¤ê³ , <strong>verify</strong>í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ë¼.</li>
</ul>

<h3 id="given">Given</h3>

<ul>
  <li>
    <p>ì•„ë˜ <strong>ì¸ì</strong>ëŠ” cmdë¥¼ í†µí•´ì„œ ì§ì ‘ ì…ë ¥ì„ ë°›ê²Œ ë˜ëŠ” codeì´ë‹¤</p>

    <ul>
      <li>dataset_type[str] -&gt; two moon ë˜ëŠ” iris <strong>ë°ì´í„°ë¥¼ ê²°ì •í•˜ëŠ” ì¸ì</strong></li>
      <li>feature_num[int] -&gt; ì…ë ¥ ë°ì´í„°ì˜ <strong>feature ê°œìˆ˜</strong></li>
      <li>class_num[int] -&gt; ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” <strong>í´ë˜ìŠ¤ ê°œìˆ˜</strong></li>
      <li>hidden_layer_num[int] -&gt; <strong>hidden layerì˜ ê°œìˆ˜</strong></li>
      <li>hidden_layer_neurons[str] -&gt; ê° hidden layerì˜ <strong>ë‰´ëŸ°ì˜ ê°œìˆ˜</strong>. stringìœ¼ë¡œ ë°›ì•„ int í˜•ì‹ì˜
arrayë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•˜ì—¬ì•¼ í•¨ ex) â€œ10 10 10â€  -&gt;  [10, 10, 10]</li>
      <li>epochs[int] -&gt; <strong>í•™ìŠµ íšŸ</strong>ìˆ˜ (epoch)</li>
      <li>learning_rate[float] -&gt; <strong>learning rate ìˆ˜ì¹˜</strong></li>
      <li>train_data_dir[str] -&gt; <strong>í•™ìŠµ ë°ì´í„° íŒŒì¼ ìœ„ì¹˜</strong></li>
      <li>train_data_num[int] -&gt; <strong>í•™ìŠµ ë°ì´í„° ê°œìˆ˜</strong></li>
      <li>validation_data_dir[str] -&gt; <strong>validation ë°ì´í„° íŒŒì¼ ìœ„ì¹˜</strong></li>
      <li>validation_data_num[int] -&gt; <strong>validation ë°ì´í„° ê°œìˆ˜</strong></li>
      <li>test_data_dir[str] -&gt; <strong>test ë°ì´í„° íŒŒì¼ ìœ„ì¹˜</strong></li>
      <li>test_data_num[int] -&gt; <strong>test ë°ì´í„° ê°œìˆ˜</strong></li>
      <li>test_output_dir[str] -&gt; test ë°ì´í„°ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ <strong>ì¶œë ¥ ê²½ë¡œ</strong></li>
    </ul>
  </li>
  <li>
    <p>Class</p>

    <ul>
      <li>
        <p>Layer</p>

        <table>
          <thead>
            <tr>
              <th>V or M</th>
              <th>name</th>
              <th>type</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>V</td>
              <td>in_dimentsion</td>
              <td>int</td>
              <td>ì…ë ¥ ì°¨ì›</td>
            </tr>
            <tr>
              <td>V</td>
              <td>out_dimension</td>
              <td>int</td>
              <td>ì¶œë ¥ ì°¨ì›</td>
            </tr>
            <tr>
              <td>V</td>
              <td>weight</td>
              <td>float[][]</td>
              <td>íŒŒë¼ë¯¸í„°</td>
            </tr>
            <tr>
              <td>V</td>
              <td>bias</td>
              <td>float[]</td>
              <td>íŒŒë¼ë¯¸í„°</td>
            </tr>
            <tr>
              <td>V</td>
              <td>input</td>
              <td>float[][]</td>
              <td>forward í•¨ìˆ˜ì—ì„œ ë°›ì€ ì…ë ¥ì„ ì €ì¥(Backwardì—ì„œ ì‚¬ìš©)</td>
            </tr>
            <tr>
              <td>V</td>
              <td>output</td>
              <td>float[][]</td>
              <td>í•¨ìˆ˜ì—ì„œ ë°›ì€ ì…ë ¥ì„ ì €ì¥(Backwardì—ì„œ ì‚¬ìš©)</td>
            </tr>
            <tr>
              <td>M</td>
              <td>forward(float[][])</td>
              <td>float[][]</td>
              <td>ì…ë ¥ì„ ë°›ì•„ ì¶œë ¥ ê°’ì„ return</td>
            </tr>
            <tr>
              <td>M</td>
              <td>backward(float[][],float)</td>
              <td>float[][]</td>
              <td>ì…ë ¥ ê°’ì„ í†µí•´ ì–»ì€ ê°’ê³¼ ì´ì „ gradient ê°’ì„ í†µí•´ weightì™€ ì „ë‹¬í•  Gradientë¥¼ ì¶œë ¥í•œë‹¤</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>Loss</p>

        <table>
          <thead>
            <tr>
              <th>V or M</th>
              <th>Name</th>
              <th>Type</th>
              <th>Descroption</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>V</td>
              <td>logit</td>
              <td>float[][]</td>
              <td>ì‹ ê²½ë§ì˜ ìµœì¢… ê²°ê³¼ ê°’ (forwardì—ì„œ ì…ë ¥ ë°›ì•„ ì €ì¥)</td>
            </tr>
            <tr>
              <td>V</td>
              <td>label</td>
              <td>float[][]</td>
              <td>forwardì—ì„œ ì…ë ¥ ë°›ì•„ ì €ì¥</td>
            </tr>
            <tr>
              <td>M</td>
              <td>forward(float[][])</td>
              <td>float[][]</td>
              <td>logitê³¼ labelì„ ì…ë ¥ ë°›ì•„ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¶œë ¥</td>
            </tr>
            <tr>
              <td>M</td>
              <td>backward(void)</td>
              <td>float[][]</td>
              <td>ë‹¤ìŒ layerì— ì „ë‹¬í•  Gradientë¥¼ logitê³¼ labelì„ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>Dataloader</p>

        <table>
          <thead>
            <tr>
              <th>V or M</th>
              <th>Name</th>
              <th>Type</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>V</td>
              <td>file_dir</td>
              <td>string</td>
              <td>ë°ì´í„° íŒŒì¼ ê²½ë¡œ</td>
            </tr>
            <tr>
              <td>V</td>
              <td>mode</td>
              <td>string</td>
              <td>train, validation,test ëª¨ë“œë¥¼ ì§€ì •</td>
            </tr>
            <tr>
              <td>V</td>
              <td>feature_num</td>
              <td>int</td>
              <td>dataì˜ feature ê°œìˆ˜(ì°¨ì›)</td>
            </tr>
            <tr>
              <td>V</td>
              <td>class_num</td>
              <td>int</td>
              <td>ë¶„ë¥˜í•  í´ë˜ìˆ˜ ê°œìˆ˜</td>
            </tr>
            <tr>
              <td>V</td>
              <td>data_num</td>
              <td>int</td>
              <td>ë°ì´í„° ê°œìˆ˜</td>
            </tr>
            <tr>
              <td>V</td>
              <td>data</td>
              <td>float[][]</td>
              <td>í•™ìŠµ data</td>
            </tr>
            <tr>
              <td>V</td>
              <td>lable</td>
              <td>float[][]</td>
              <td>test ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ</td>
            </tr>
            <tr>
              <td>M</td>
              <td>read(void)</td>
              <td>void</td>
              <td>ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ dataì™€ labelì— ê°ê° ì €ì¥í•¨ (test modeì—ì„œëŠ” labelì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>Mlp (Multi-Layer Perceptron)</p>

        <table>
          <thead>
            <tr>
              <th>V or M</th>
              <th>Name</th>
              <th>Type</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>V</td>
              <td>layer</td>
              <td>Layer[]</td>
              <td>Layer ì˜¤ë¸Œì íŠ¸ array-hidden layer ê°œìˆ˜ì— ë”°ë¼ ê¸¸ì´ ë³€ë™</td>
            </tr>
            <tr>
              <td>V</td>
              <td>loss</td>
              <td>Loss</td>
              <td>Loss ì˜¤ë¸Œì íŠ¸</td>
            </tr>
            <tr>
              <td>V</td>
              <td>train_dataset</td>
              <td>Dataloade</td>
              <td>trainì— ì‚¬ìš©í•  Dataloader ì˜¤ë¸Œì íŠ¸</td>
            </tr>
            <tr>
              <td>V</td>
              <td>train_data</td>
              <td>float[][]</td>
              <td>train_datasetì—ì„œ ì–»ì€ ë°ì´í„°ë¥¼ ì €ì¥</td>
            </tr>
            <tr>
              <td>V</td>
              <td>train_label</td>
              <td>float[][]</td>
              <td>train_datasetì—ì„œ ì–»ì€ ë ˆì´ë¸” ì €ì¥(one_hot)</td>
            </tr>
            <tr>
              <td>V</td>
              <td>validation_dataset</td>
              <td>Dataloader</td>
              <td>validationì— ì‚¬ìš©í•  dataloaderì˜¤ë¸Œì íŠ¸</td>
            </tr>
            <tr>
              <td>V</td>
              <td>validation_data</td>
              <td>float[][]</td>
              <td>validation_datasetì—ì„œ ì–»ì€ ë°ì´í„°ë¥¼ ì €ì¥</td>
            </tr>
            <tr>
              <td>V</td>
              <td>validation_label</td>
              <td>float[][]</td>
              <td>validation_datasetì—ì„œ ì–»ì€ ë ˆì´ë¸” ì €ì¥(one_hot)</td>
            </tr>
            <tr>
              <td>V</td>
              <td>epochs</td>
              <td>int</td>
              <td>epoch</td>
            </tr>
            <tr>
              <td>v</td>
              <td>learning_rate</td>
              <td>float</td>
              <td>learning rate</td>
            </tr>
            <tr>
              <td>M</td>
              <td>read_dataset(Dataloader)</td>
              <td>void</td>
              <td>ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ dataì™€ labelì— ê°ê° ì €ì¥ í•¨</td>
            </tr>
            <tr>
              <td>M</td>
              <td>add_layer(layer)</td>
              <td>void</td>
              <td>layerì— Layer ì˜¤ë¸Œì íŠ¸ë¥¼ ì¶”ê°€í•¨</td>
            </tr>
            <tr>
              <td>M</td>
              <td>visulaize_layer(void)</td>
              <td>void</td>
              <td>layer ê³„ì¸µ êµ¬ì¡°ë¥¼ ì¶œë ¥</td>
            </tr>
            <tr>
              <td>M</td>
              <td>train(void)</td>
              <td>void</td>
              <td>ê° epochë§ˆë‹¤ lossì™€ ì •í™•ë„ë¥¼ ì¶œë ¥</td>
            </tr>
            <tr>
              <td>M</td>
              <td>validation(void)</td>
              <td>void</td>
              <td>validation setì— ëŒ€í•´ì„œ lossì™€ ì •í™•ë„ë¥¼ ì¶œë ¥</td>
            </tr>
            <tr>
              <td>M</td>
              <td>predict(Dataloader,str)</td>
              <td>void</td>
              <td>ì…ë ¥í•œ Dataloader(testì„)ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<h3 id="condition">Condition</h3>

<ul>
  <li>ê° datasetì— ëŒ€í•´ì„œ í•™ìŠµì´ ì˜ë˜ëŠ” hyper-parameter(epochs,learning-rate, hidden layer ì¸µìˆ˜, neuronì˜ ê°œìˆ˜ ë“±) ë¥¼ ì°¾ëŠ” ì‹¤í—˜ì— ëŒ€í•œ ê³¼ì • ë° ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì œì¶œí•´ì•¼ í•¨. í•„ìˆ˜ ì‹¤í—˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Epochs</th>
      <th>Learning Rate (\(\eta\))</th>
      <th>Hidden Layer ê°œìˆ˜</th>
      <th>Neuron ê°œìˆ˜(Perceptron)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>100</td>
      <td>0.01</td>
      <td>1</td>
      <td>10</td>
    </tr>
    <tr>
      <td>500</td>
      <td>0.05</td>
      <td>2</td>
      <td>10,10</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.01</td>
      <td>3</td>
      <td>30,100,30</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.03</td>
      <td>2</td>
      <td>100,100</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>ìœ„ ì¡°ê±´ì— ë”°ë¼ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ê¸°ì…í•˜ê³  ìì‹ ì´ ì°¾ì€ ìµœì ì˜ ê°’ì„ ì°¾ëŠ” ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì‘ì„±í•´ í•¨</li>
  <li>í•™ìŠµí•œ ë„¤íŠ¸ì›Œí¬ë¡œ ê° datasetì— ì¡´ì¬í•˜ëŠ” test dataset(ì •ë‹µ ë ˆì´ë¸”ì´ ì—†ëŠ”)ì— ëŒ€í•´ [Test ê²°ê³¼ ì˜ˆì‹œ]ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ê³¼ì œ ì œì¶œ ì‹œ ì†ŒìŠ¤ì½”ë“œì™€ í•¨ê¼ ì œì¶œí•´ì•¼ í•¨</li>
</ul>

<hr />

<h3 id="ref">Ref</h3>

<ul>
  <li><a href="[https://lovit.github.io/machine%20learning/2018/04/27/synthetic_dataset/](https://lovit.github.io/machine learning/2018/04/27/synthetic_dataset/)">LOVITxDATA SCIENCE</a>
    <ul>
      <li>soydaya, ë³µì¡í•œ ì¸ê³µ ë°ì´í„° ìƒì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤â€¦..<strong>Skit-Learn</strong></li>
    </ul>
  </li>
  <li><a href="https://www.youtube.com/watch?v=bUNqn1G1O7E">í¸ë¯¸ë¶„ê³¼ Gradientì˜ ê¸°í•˜í•™ì  ì˜ë¯¸</a></li>
  <li><a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction">Khan: ChainRule</a></li>
  <li><a href="http://researchhubs.com/post/maths/fundamentals/basic-of-matrix.html">Matrixì˜ ê¸°ë³¸ ì§€ì‹</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">í•˜ë‹¤ë§ˆë“œ ê³±: \(\odot\)</a></li>
  <li><a href="[https://ko.wikipedia.org/wiki/%ED%96%89%EB%A0%AC_%EA%B3%B1%EC%85%88](https://ko.wikipedia.org/wiki/í–‰ë ¬_ê³±ì…ˆ)">Matrixì˜ ê³±</a></li>
  <li>[1-07. Multi Layer Perceptron ì´ì •ë¦¬](</li>
</ul>
:ET