---
layout: post
title: Multi-layer Perceptron Description
categories: [AI]
tags: [MLP, POSTECH]
---
# Multi-layer Perceptron

## Objective 

* ë³¸ ê³¼ì œì—ì„œëŠ” Multi-Layer Perceptronì„ êµ¬í˜„í•˜ì—¬ Two Moon Dataset ê³¼ Iris Dataset ë“¤ì„ í•™ìŠµí•˜ê³  ê²°ê³¼ë¥¼ ì‚°ì¶œí•œë‹¤. ì´ë¥¼ í†µí•´ C++ì˜ ë‹¤ì–‘í•œ Class ê¸°ë²•ì„ ì—°ìŠµí•œë‹¤

## Background

> Perceptron, Gradient Descent, Multi-Layer Perceptron Error, backpropagation

### Perceptron

* Percetronì€ Neural Networkì˜ ê°€ì¥ ê¸°ë³¸ ë‹¨ìœ„ì´ë‹¤.

* ìˆ˜ìƒëŒê¸°ë¥¼ í†µí•´ ë“¤ì–´ì˜¨ ë‹¤ìˆ˜ì˜ ì‹ í˜¸($$x$$)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ *ë³€ì¡°($$\text{activation } \sigma$$)*í•˜ì—¬ ì¼ì • thresholdë¥¼ ë„˜ê¸°ëŠ” ê²½ìš°, ì‹ í˜¸ë¥¼ ì¶œë ¥($$\hat{y}$$)í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤.

* Perceptron: **$$\hat{y} = \sigma(\Sigma{w_ix_i+b}) = \sigma(WX+b)$$**

  > * ì…ë ¥(feature): $$X=(x_1,x_2,...,x_n)$$
  >
  > * ì¶œë ¥ : $$\hat{y}$$
  >
  > * Model Parameter
  >   * ê°€ì¤‘ì¹˜: $$W = (w_1,w_2,...,w_n)^T$$
  >   * bias: $$b$$
  >
  > 

* $$\sigma(x) = \frac{1}{1+e^{-x}}$$

  > sigmoid functionì˜ ê°’ì€ 0ì—ì„œ 0.5, ê·¸ë¦¬ê³  ìŒìˆ˜ì™€ ì–‘ìˆ˜ì¼ ë•Œ ê°ê° 0.5ë³´ë‹¤ ì‘ê³ 
  > 0.5ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ë¯€ë¡œ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Trueì™€ Falseë¡œ ë‚˜ëˆ„ëŠ” activationì— ì í•©í•œ í•¨ìˆ˜ì´ë‹¤.

### Gradient Descent

> Perceptronì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ Gradient Descent ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤.

* Loss Function ëª©ì  í•¨ìˆ˜

  * Perceptionì˜ ê²°ê³¼ê°’ê³¼ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì •ë‹µê³¼ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì£¼ëŠ” í•¨ìˆ˜.

  * ì •ë‹µê³¼ ê±°ë¦¬ê°€ ë©€ë©´ í° ê°’ì„, ê°€ê¹Œìš°ë©´ ì‘ì€ ê°’ì„ ì¶œë ¥í•˜ì—¬ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.

  * ê°€ì¥ ê°„ë‹¨í•œ í•¨ìˆ˜ê°€ Mean Square Errorì´ë‹¤

    > **$$\text{loss function}: L(y,\hat{y}) = \frac{(\hat{y}-y^2)}{2}$$**

    * **yëŠ” Data ì‹¤ì œ ë ˆì´ë¸”(ì •ë‹µ**). ì¦‰, ëª©ì  í•¨ìˆ˜ì˜ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì •ë‹µì— ê°€ê¹Œì›Œì§„ë‹¤. 

  * ì´ë•Œ, ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì˜ ê°’ì„ ìµœì†Œë¡œ í•˜ëŠ” *ìµœì ì˜ í•´*ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ **Gradient**ë¥¼ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆë‹¤. 

* Gradient Descent

  > ê·¸ë¦¼ ì°¸ê³ 

  * ê·¸ë¦¼ì„ ë³´ë©´, GradientëŠ” íŠ¹ì • ì§€ì (ê·¸ë¦¼ì˜ initial weight)ì˜ tangential slopeì„ ë‚˜íƒ€ë‚¸ë‹¤
  * ì´ë•Œ Model Parameterë¥¼ Gradient Negative ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•œë‹¤ë©´, ì´ê²ƒì„  loss function ê°’ì„ ìµœì†Œë¡œ ê°–ê²Œ í•˜ëŠ” ë°©ë²•ì´ë¼ëŠ” ê²ƒì„ ì§ì‘í•  ìˆ˜ ìˆë‹¤.
  * ì´ëŸ° ê³¼ì •ì„ **"í•™ìŠµ"**ì´ë¼ê³  í•œë‹¤
    * Model Parameterì„ Gradientê°€ Descentí•˜ê²Œ Updateí•˜ì—¬ loss functionì˜ ê°’ì´ ìµœì†Œë¡œ ê°–ê²Œ í•˜ëŠ” ê²ƒ
  * ì ê·¸ëŸ¼ ì´ì œ, gradient descentë¥¼ í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ parameterë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ êµ¬í•˜ì

### Model Parameter 

> Model Parameter
>
> - ê°€ì¤‘ì¹˜: $$W = (w_1,w_2,...,w_n)^T$$
> - bias: $$b$$

* Chain Ruleì„ í†µí•´ì„œ ê° Parameterì— ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê°’ì„ êµ¬í•œë‹¤

* $$a =\Sigma_iw_ix_i+b$$ë¼ê³  ì •ì˜ í• ë•Œ, **Gradient**ëŠ” ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ êµ¬í•œë‹¤

  > * $$a =\Sigma_iw_ix_i+b$$
  > * L: loss function $$\text{loss function}: L(y,\hat{y}) = \frac{\hat{y}-y^2}{2}$$
  > * $$\sigma(a) = \frac{1}{1+e^{-a}}$$
  > * $$\hat{y}$$=$$\sigma(XW+b)$$

$$
\frac{ğœ•L}{ğœ•\hat{y}}=\hat{y}-y
$$

$$
\frac{ğœ•L}{ğœ•a}=\sigma(a)(1-\sigma(a))
$$

$$
\frac{ğœ•a}{ğœ•w_i}=x_i,\frac{ğœ•a}{ğœ•b}=1
$$

$$
âˆ´\frac{ğœ•L}{ğœ•w_i}=\frac{ğœ•a}{ğœ•\hat{y}}\times \frac{ğœ•a}{ğœ•w_i}=(\hat{y}-y)\sigma(1-\sigma(a))x_i
$$

$$
âˆ´\frac{ğœ•L}{ğœ•b}=\frac{ğœ•L}{ğœ•\hat{y}}\times \frac{ğœ•\hat{y}}{ğœ•a}\times \frac{ğœ•a}{ğœ•b}=(\hat{y}-y)\sigma(1-\sigma(a))
$$



* ìœ„ì—ì„œ ê²°ê³¼ì ìœ¼ë¡œ ë‚˜ì˜¨ **Gradient** ì‹ì˜ ê°’ì´ ìµœì†Œê°’ì„ ê°–ë„ë¡í•˜ëŠ” **Model Parameter**ì„ êµ¬í•´ì•¼ í•œë‹¤. 

* ë¬¸ì œëŠ”, ì´ëŒ€ë¡œ Gradient ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´, ìˆ˜ë ´í•˜ì§€ ì•Šê³  **ì§„ë™**ì„ í•œë‹¤

  > ê³ ë¡œ, Learning Rate $$\eta$$ ë¼ëŠ” **hyper-parameter**ì„ ì„¤ì •í•œë‹¤

### Hyper-Parameter

* $$\eta$$ ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´í•˜ëŠ”ë° ì˜¤ëœì‚¬ê°„ì´ ê±¸ë¦¬ê³ , ë„ˆë¬´ í¬ë©´ ë°œì‚°í•˜ê¸° ë•Œë¬¸ì— ì ë‹¹í•œ ê°’ì„ ê°€ì ¸ì•¼ í•œë‹¤

  > Parameterê°€  updateí•˜ê¸° ìœ„í•´ì„œ  Old Valueì—ì„œ valueì˜ Gradientì„ Hyper-Parameterì™€ ê³±í•´ì„œ ëº€ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤

$$
w_i^{new} = w_i^{old}-\eta\frac{ğœ•L}{ğœ•w_i^{old}}
$$

$$
b^{new} = b^{old} - \eta\frac{ğœ•L}{ğœ•b^{old}}
$$



> ì‚¬ì‹¤ paramterì„ êµ¬í•˜ê¸° ìœ„í•´ Gradient í•¨ìˆ˜ë¥¼ Chain Ruleì´ ì•„ë‹Œ ë” ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ, ë’¤ì— ë‚˜ì˜¬ **Error Backpropagation** ë•Œë¬¸ì´ë‹¤

* ë” í¸í•œ ê³„ì‚°<sub>(Multi-Layerì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ë„ ìˆìŒ)</sub> ì„ ìœ„í•´ **Vector**ì„ ì‚¬ìš©í•˜ì—¬ ì¶•ì•½ëœ Gradientì‹ì„ ì‚¬ìš© í•  ê²ƒì´ë‹¤.($$\nabla$$ëŠ” í¸ë¯¸ë¶„ ê¸°í˜¸ë¥¼ ëŒ€ì‹œ ì‚¬ìš©í•œë‹¤)

  > í¸ë¯¸ë¶„??

$$
\nabla_{\hat{y}}L = \hat{y}-y
$$

$$
\nabla_{\hat{y}}L =\sigma(a)(1-\sigma(a))
$$

$$
\nabla_{w}a=X^T, \nabla_ba = 1
$$

$$
âˆ´\nabla_wL=\nabla_{\hat{y}}L\times\nabla_a\hat{y}\times\nabla_wa=(\hat{y}-y)\sigma(a)(1-\sigma(a))X^T
$$

$$
âˆ´\nabla_bL=\nabla_{\hat{y}}L\times\nabla_a\hat{y}\times\nabla_ba=(\hat{y}-y)\sigma(a)(1-\sigma(a))
$$

> TëŠ” Matrixì˜ transposeì´ë‹¤....ê°€ë¡œë¥¼ ìƒˆë¡œë¡œ, ì„¸ë¡œë¥¼ ê°€ë¡œë¡œ

* ë‹¤ìŒ ì‹ì€ $$\nabla$$ë¥¼ ì ìš©í•œ hyper-paramterì´ë‹¤

$$
  w^{new} = w^{old}-\eta\nabla_{w^{old}}L
$$

$$
  b^{new} = b^{old} - \eta\nabla_{b^{old}}L
$$

  

> ì—¬ê¸°ê¹Œì§€ëŠ” ì˜¬ë°”ë¥¸ Hyper Parameterì„ êµ¬í•˜ê¸° ìœ„í•˜ì—¬ Gradientì„ êµ¬í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ í•˜ì˜€ë‹¤. ì´ì œ Perceptronì˜ ê°œë…ì´ í™•ì¥ëœ **Multi-Layer Perceptron**ì˜ ê°œë…ì— ëŒ€í•´ ì´ì•¼ê¸° í•˜ì

### Multi-Layer Perceptron

> * ê°„ë‹¨í•˜ê²Œ, Multi-Layerì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ê·¸ êµ¬ì„±ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°ë¥¼ í•˜ì
>  * Muli-Layer Perceptionì€ ì—¬ëŸ¬ê°œì˜ Perceptionì„ stackingí•œ ê²ƒì„ ë§í•œë‹¤.  Stackingì„ í•˜ê¸° ë•Œë¬¸ì— ì¶œë ¥ê°’ì´ Scalarê°€ ì•„ë‹Œ Vector í˜•íƒœë¡œ ë‚˜ì™€ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤.
>   * ë˜í•œ, Perceptionì€ linear functionì´ê¸° ë–„ë¬¸ì—, nonlinearí•œ ì •ë³´ë¥¼ classifyí•˜ì—¬ì„œ Multi-Layer Perceptionì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•˜ë‹¤.

* ê·¸ë¦¼ ì°¸ì¡°

  > ê° ì›ì€ í•˜ë‚˜ì˜  perceptionì„ ì˜ë¯¸í•œë‹¤. 
  >
  > * Wì™€ bëŠ” parameterê°€ ëœê³ , Hiddent LayerëŠ” ê³„ì¸µì ì¸ ê´€ì ì—ì„œ Perceptionì˜ ì§‘í•©ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤

* **Hidden Layer 1**ì˜ ìˆ˜ì‹ì´ë‹¤

  * $$m$$ì€ hidden layerì˜ perceptron ê°œìˆ˜ì´ë‹¤

    $$
    h_{1i}=\sigma(XW_{1i}+b_{1i}), i = \{1,....m\}
    $$

    $$
    h_1=(h_{11},h_{_12},...h_{1m})
    $$

    

  > $$h_n$$ì€ hidden layer nì„ ë‚˜íƒ€ë‚¸ë‹¤

* Hidden Layer 1ì˜ ì¶œë ¥ì€ Hidden Layer 2ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤. ë‹¤ìŒì‹ì€, **Hidden Layer 2**ì˜ ìˆ˜ì‹ì´ë‹¤

  * $$m$$ì€ ì—­ì‹ hidden layer 2ì˜ perceptron ê°œìˆ˜ì´ë‹¤

    $$
    h_{2j}=\sigma(h_1W_{2j}+b_{2j}),j=\{1,...j\}
    $$

* **Matrix Multiplication**ì„ ì‚¬ìš©í•˜ì—¬ í‘œí˜„í•˜ë©´ ë” ê°„ê²°í•˜ê²Œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤

* **Hidden Layer 1**

  > * $$W_{1nm}$$ì´ë‹¤, ì¦‰, ê°€ì¥ ì•ì˜ 1ì€ $$W_1$$ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ë‹¤.$$b$$ë„ ë™ì¼í•˜ë‹¤....Don't Confuse
  > * $$n$$ì€ input ê°’ì˜ **ê°¯ìˆ˜ì´ë‹¤**

$$
W_1=\begin{bmatrix}
  W_{111} & \cdots & W_{1n1}\\
  \vdots & \ddots &\vdots \\
  W_{11m} & \cdots & W_{1nm}\\
  \end{bmatrix}
  =
  \begin{bmatrix}
  W_{11} & \cdots & W_{1m}
  \end{bmatrix}
$$

$$
b_1=\begin{bmatrix}
b_{11} & \cdots & b_{1m}
\end{bmatrix}
$$

$$
h_1 =\begin{bmatrix}
\sigma(XW_{11}+b_{11}) & \cdots & \sigma(XW_{1m}+b_{1m})
\end{bmatrix}
=\sigma(XW_{1}+b_{1})
$$

* **Hidden Layer 2**ë„ ê°™ì€ ë°©ë²•ìœ¼ë¡œ í‘œí˜„í•œë‹¤

$$
\hat{y}=h_2=\sigma(h_1W_2+b_2)
$$

$$
\hat{y}=(\hat{y_1},\hat{y_2},...\hat{y_l})
$$



* **ë ˆì´ë¸” $$y$$**ì˜ í‘œí˜„ ë²•

  * ì•ì—ì„œ $$y$$ëŠ” dataì˜ ì‹¤ì œ ë ˆì´ë¸”(ì •ë‹µ)ì´ë¼ê³  í–ˆë‹¤
  * ì´ $$y$$ë¥¼ í‘œí˜„ í• ë•Œ, *one-hot encoding*ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.
  * $$y$$ê°€ ì´ 10ì˜ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ, 3ë²ˆ classì˜ lableì„ ê°€ì§„ë‹¤ê³  í•˜ë©´ $$y$$ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤

  $$
  y=(0,0,1,0,0,0,0,0,0,0)
  $$

  > í•´ì„ì„ í•˜ë©´, lableì„ ê°–ê³  ìˆì„ ìˆ˜ ìˆëŠ” $$y$$ì˜ 3ë²ˆì§¸ classì— lableì´ ìˆë‹¤ëŠ” ë§ì´ë‹¤??...ê·¸ë˜ì„œ ë¹„êµ í•  ìˆ˜ ìˆë‹¤ëŠ” ë§??

  * ìµœì¢… ê²°ê³¼ê°’ì¸ $$\hat{y}$$ì—ì„œ ê°€ì¥ ë†’ì€ ê°’ì„ ì¶”ì¶œí•˜ë©´ ê·¸ ê°’ê³¼  lableì„ ë¹„êµ í•  ìˆ˜ ìˆë‹¤. 
  * ë¹„êµí• ë•ŒëŠ”, ê°€ì¥ ë†’ì€ ê°’, argmaxì¸ $$\hat{y}$$ì™€ lableê³¼ ë¹„êµí•œë‹¤

> lable $$y$$ê³¼ $$\hat{y}$$ì˜ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ì„œ
>
> * ê·¸ë¦¼ 5ë¥¼ ì°¸ê³ í•˜ë¼. Inputì€ hidden layerì„ í†µí•´ì„œ outputì„ ë§Œë“¤ì–´ ë‚¸ë‹¤.
>   * hidden layer ì•ˆì€ perceptronìœ¼ë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆê³ ,  input ê°’ê³¼ output ê°’ì˜ ê°œìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆê¸° ë–„ë¬¸ì—, hidden layer 1ì—ì„œëŠ” ë‹¤ìˆ˜ì˜ outputì„ ë§Œë“œëŠ”ë°, ì™œ hidden layer 2 ëŠ” í•˜ë‚˜ì˜  outputë§Œ ë§Œë“œëƒëŠ” ì§ˆë¬¸ì€ í•˜ì§€ ë§ì
> * ì´ë•Œ, hidden layerì„ í†µí•´ì„œ ë§Œë“  ê°’, ì¦‰, outputì´ $$\hat{y}$$ì´ê³ , $$y$$ëŠ”  lableë¡œì¨ ì •ë‹µì´ë‹¤.(ì£¼ì–´ì§„ ê°’ì¸ê°€??)
> * ê³ ë¡œ, $$\hat{y}$$ì™€  $$y$$ëŠ” ê°™ì€ ì„±ì§ˆ(?)ì˜  outputì´ê¸° ë–„ë¬¸ì— ë¹„êµê°€ ê°€ëŠ¥í•˜ë‹¤

* **Classification**
  * classificationì„ í†µí•´ì„œ $$\hat{y}$$ë¥¼ ì¶œë ¥í•œë‹¤ê³  í•œë‹¤.
  * ì¦‰,  Hidden Layerë“¤ì„ ë§í•˜ëŠ” ê²ƒì´ë‹¤??

### Error Backpropagation

> * ì ê¹ ì •ë¦¬ë¥¼ í•´ë³´ì
> * ì§€ê¸ˆ ê¹Œì§€ëŠ” Parceptronì˜ ê°œë…ê³¼, ì˜¬ë°”ë¥¸ Parceptron, ì¦‰, model paramterë¥¼ êµ¬í•˜ê¸° ìœ„í•´ multil-layerì˜ ê°œë…ì„ ë°°ì› ë‹¤. Loss Functionrê³¼ Percentron ì‹ìœ¼ë¡œ ë¶€í„° Model Parameterì„ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë°°ì› ìœ¼ë‚˜, **ì–´ë–»ê²Œ í•˜ë©´ *ì¢‹ì€* Model parameterë¥¼ êµ¬í•˜ëŠ”ì§€, ì¦‰ Updateë¥¼ í•˜ì—¬ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì•„ì§ ì•ˆë°°ì› ë‹¤.** 
> * *ì¢‹ì€* Model Parameterì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ **í•™ìŠµ ë°©ë²•**ì´ë¼ê³  ì´ì•¼ê¸° í•  ìˆ˜ ìˆë‹¤. **Error Backpropagation**ì„ í†µí•´ì„œ í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ solutionì— ëŒ€í•´ ë°°ìš¸ ê²ƒ ì´ë‹¤.  (Gradient í•¨ìˆ˜ë¥¼ Chai Ruleë¡œ í‘¼ ì´ìœ )

* Chain Ruleì„ ì‚¬ìš©í•˜ì—¬ ê° ê³„ì¸µì— ìˆëŠ” parameterì˜ Gradientë¥¼ êµ¬í•˜ì—¬ ë’¤ì—ì„œ ë¶€í„° **ìˆœì°¨ì ìœ¼ë¡œ** Updateí•˜ëŠ” ë°©ë²•ì´ë‹¤.

  * Chain Ruleì„ í†µí•´ì„œ ë’¤ì—ì„œ ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ êµ¬í•˜ëŠ” ì´ìœ ëŠ”,  Multi-Layer Perceptionì˜ ì •ì˜ëŠ” ìœ„ì—ì„œ ë°°ìš´ ê²ƒ ê°™ì´ ëª…í™•í•˜ê²Œ ë‚´ë¦´ ìˆ˜ ìˆì§€ë§Œ, ê²Œì¸µì„ ìŒ“ê²Œ ë˜ë©´ Loss Functionì— ëŒ€í•œ ë§ˆë•…í•œ í•™ìŠµì„ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.
  * ì¦‰, í•™ìŠµì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ì´ë‹¤

  > Parameterì„ í†µí•´ Gradientë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.
  >
  > ê·¸ë¦¼ 5, ì²˜ëŸ¼ Hidden LayerëŠ” 2ê°œ ë¶„ì•„ë‹¤.(ë§ˆì§€ë§‰ Hidden LayerëŠ” 2ë²ˆì¨° ê²ƒ ì´ë‹¤)

  $$
  \text{Given: }a_2 = h_2W_2+b_2
  $$

  $$
  L(y,\hat{y})=(\hat{y}-y)^2
  $$

  $$
  \nabla_{\hat{y}}L=\hat{y}-y
  $$

  $$
  \nabla_{a_2}\hat{y}=\sigma(a_2)(1-\sigma(a_2))
  $$

  $$
  \nabla_{w_2}a_2=h_1^T,\nabla_{b_2}a_2=1=(1,....,1)^T
  $$

  > ì—¬ê¸°ì„œ 1ì€ vectorì´ë‹¤

  $$
  âˆ´\nabla_{w_2}L=\nabla_{\hat{y}}L\odot\nabla_{a_2}\hat{y}\times\nabla_{w_1}a_2=(\hat{y}-y)\odot\{\sigma(a_2)(1-\sigma(a_2))\}\times h_{1}^T
  $$

  $$
  âˆ´\nabla_{b_2}L=\nabla_{\hat{y}L}\times \nabla_{b}a_2=(\hat{y}-y)\odot\{\sigma(a_2)\{1-\sigma(a_2\}\}\times 1
  $$

  * ìœ„ì˜ ì‹ì‚¬ìš©í•˜ì—¬, **Hidden Layer 2ì˜ Parameterì„ Update** í•˜ì˜€ë‹¤
    $$
    W_2^{new}=W_{2}^{old}-\eta\nabla_{w_2^{old}}L
    $$

    $$
    b_2^{new}=b_2^{old}-\eta\nabla_{b_2^{old}}L
    $$

    > * ìœ„ ì‹ì—ì„œëŠ” ì£¼ì–´ì§„ parameterë¥¼ Hidden Layer 2ì— ê¸°ì¤€ì—ì„œ ìƒˆë¡œìš´ parameterë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•˜ì˜€ë‹¤.
    >
    > * Hyper-Parameterë¥¼ êµ¬í• ë•Œì™€ ê°™ì€ ë°©ë²•ì´ë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” Hidden Layer 2ì—ì„œ ë¶€í„° ê±°ê¾¸ë¡œ ê°€ê¸° ë•Œë¬¸ì—, ì´ì œ ìš°ë¦¬ëŠ” Hidden Layer 1ì— ëŒ€í•œ parameterì„ êµ¬í•  ìˆ˜ ìˆë‹¤

  * **Hidden Layer 1ì˜ Parameterì„ Update** í•´ì£¼ì–´ì•¼ í•œë‹¤.

    * Parameterì„ Updateí•˜ê¸° ìœ„í•´ì„œëŠ” $$h_1$$ì— ëŒ€í•œ Gradientë¥¼ chain ruleì„ ì‚¬ìš©í•˜ì—¬  $$\nabla_{w_1}{L}$$ì™€ $$\nabla_{b_1}L$$ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.
    * $$a_2$$ì— ëŒ€í•œ Gradientê¹Œì§€ êµ¬í•œ ë¶€ë¶„ì— ì ìš© í•´ì•¼  $$\nabla_{w_1}{L}$$ì™€ $$\nabla_{b_1}L$$ì„ êµ¬í•  ìˆ˜ ìˆëŠ”ë°, ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ”  $$\nabla_{h_1}a_2$$ì„ ì•Œì•„ì•¼ í•œë‹¤.

$$
\nabla_{h_1}a_2=W_{2}^T
$$

$$
\nabla_{h_1}L = \nabla_{\hat{y}}L\odot\nabla_{a_2}{\hat{y}} \times \nabla_{h_1}a_2=(\hat{y}-y)\odot\{\sigma(a_2)\{1-\sigma(a_2)\}\}\times W_2^T
$$

> $$\nabla_{h_1}a_2$$ì„ êµ¬í–ˆìœ¼ë‹ˆ,  $$\nabla_{w_1}{L}$$ì™€ $$\nabla_{b_1}L$$ êµ¬í•˜ì—¬ Hidden Layer 1ì˜ Parameterì„ Updateí•˜ì

$$
\nabla_{a_1}h1=\sigma(a_1)(1-\sigma(a_1))
$$

$$
\nabla_{w_1}a_1=X^T, \nabla_{b_1}a_1= 1=(1,....,1)^T
$$

$$
\nabla_{W_1}L=\nabla_{h_1}L\odot\nabla_{a_1}h_1\times\nabla_{w_1}a_2
$$

$$
\nabla_{b_1}L=\nabla_{h_1}L\odot\nabla_{a_1}h_1\times\nabla_{b_1}a_2
$$

$$
W_{1}^{new}=W_{1}^{old}-\eta\nabla_{w_1^{old}}{L}
$$

$$
b_1^{new}=b_1^{old}-\eta\nabla_{b_1^{old}}L
$$

### Multi-Data

* ì§€ê¸ˆê¹Œì§€ëŠ” í•˜ë‚˜ì˜ Dataì— ëŒ€í•´ì„œë§Œ ì„¤ëª…ì„ í•˜ì˜€ì—ˆë‹¤. ë§Œì•½ **ë‹¤ìˆ˜ì˜ data**ì— ëŒ€í•œ í•™ìŠµì´ë¼ë©´, Gradientì˜ í•© ë˜ëŠ” í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ í•˜ë©´ëœë‹¤.  ë‹¤ìˆ˜ì˜ dataë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ í‘œê¸°í•œë‹¤

  * ë§Œì•½, í•©ì„ ì‚¬ìš©í•œë‹¤ë©´, í‰ê· ì„ ì‚¬ìš©í• ë•Œì— ë¹„í•´ì„œ **learning rate**ì¡°ì •ì„ ì‹ ê²½ ì¨ì•¼ í•œë‹¤. 

  $$
  X^i, i =\{1,2,....,N\}
  $$

  * ì´ í‘œí˜„ì—ì„œ $$i$$ëŠ” super scriptë¼ê³  í•˜ëŠ”ë°, $$i$$ë²ˆì¨° dataë¥¼ ëœ»í•œë‹¤

* super scriptë¥¼ ì ìš©í•˜ì—¬, **ì†ì‹¤ í•¨ìˆ˜**ë¥¼ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ëŠ”ë‹¤

  * ì¶œë ¥ê³¼ ë ˆì´ë¸”(ì •ë‹µ)ì—ë„ ê°™ì€ Notationì„ ì ìš©í•œ ê²ƒì´ë‹¤

$$
L(y,\hat{y})=\Sigma_{i=1}^N(\hat{y}^i-y^i)^2
$$

### Learning & Data

* ë°ì´í„° í¬ê¸°ê°€ í° ì´ë¯¸ì§€ ë°ì´í„° ë“±ì€ ëª¨ë“  ë°ì´í„°ë¥¼ í•œë²ˆì— ì—…ë°ì´íŠ¸ í•˜ê¸°ê°€ í˜ë“¤ë‹¤.
* ê³ ë¡œ **Mini Batch**ë¼ëŠ” ê°œë…ì„ ì ìš©í•œë‹¤.
  * ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ iterativeí•˜ê²Œ í•™ìŠµí•˜ëŠ” ê²ƒ. 
* ëª¨ë¸ì„ í•™ìŠµí• ë•Œ,  **epoch**ëŒë©´ì„œ í•œë‹¤
  * í•œ ë°ì´í„° ì„¸ì„ ì „ë¶€ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” iterationì„ ëœ» í•œë‹¤
* í•™ìŠµ ì‹œ, DataëŠ” **Train,validation, test**ë¡œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•œë‹¤
  *  **Train Data** ì€ í•™ìŠµì„ ìœ„í•´ ì‚¬ìš©ëœë‹¤.
  *  **Validation Data**ì€ í•™ìŠµ ê³¼ì •ì„ ê²€ì¦í•˜ëŠ”ë°, ì‚¬ìš©ë˜ëŠ”ë°, í•™ìŠµì— ì‚¬ìš©ì´ ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•˜ë‹¤. 
  *  **Test Data** ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¡œ ë³´í†µ ë ˆì´ë¸”ì„ ê³µê°œí•˜ì§€ ì•Šê³  ê³µì •í•œ ëª¨ë¸ í‰ê°€ì— ì‚¬ìš©ëœë‹¤.

## Program Explaination

### Goal

* ê·¸ë¦¼ 6ì€, **Two Moon Dataset**ì´ë‹¤.
  * ì´ ê·¸ë¦¼ì—ì„œëŠ” non-linearí•œ 2-feature(2ê°œì˜ input), 2-class(2ê°œì˜ output) datasetì„ ë³¼ ìˆ˜ ìˆë‹¤. 
* ê·¸ë¦¼ì— ì£¼ì–´ì§„ dataë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” **classifier**ë¥¼ í•™ìŠµì‹œí‚¤ê³ , **verify**í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ë¼.

### Given

* ì•„ë˜ **ì¸ì**ëŠ” cmdë¥¼ í†µí•´ì„œ ì§ì ‘ ì…ë ¥ì„ ë°›ê²Œ ë˜ëŠ” codeì´ë‹¤

  * dataset_type[str] -> two moon ë˜ëŠ” iris **ë°ì´í„°ë¥¼ ê²°ì •í•˜ëŠ” ì¸ì**
  * feature_num[int] -> ì…ë ¥ ë°ì´í„°ì˜ **feature ê°œìˆ˜**
  * class_num[int] -> ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” **í´ë˜ìŠ¤ ê°œìˆ˜**
  * hidden_layer_num[int] -> **hidden layerì˜ ê°œìˆ˜**
  * hidden_layer_neurons[str] -> ê° hidden layerì˜ **ë‰´ëŸ°ì˜ ê°œìˆ˜**. stringìœ¼ë¡œ ë°›ì•„ int í˜•ì‹ì˜
    arrayë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•˜ì—¬ì•¼ í•¨ ex) â€œ10 10 10â€  ->  [10, 10, 10]
  * epochs[int] -> **í•™ìŠµ íšŸ**ìˆ˜ (epoch)
  * learning_rate[float] -> **learning rate ìˆ˜ì¹˜**
  * train_data_dir[str] -> **í•™ìŠµ ë°ì´í„° íŒŒì¼ ìœ„ì¹˜**
  * train_data_num[int] -> **í•™ìŠµ ë°ì´í„° ê°œìˆ˜**
  * validation_data_dir[str] -> **validation ë°ì´í„° íŒŒì¼ ìœ„ì¹˜**
  * validation_data_num[int] -> **validation ë°ì´í„° ê°œìˆ˜**
  * test_data_dir[str] -> **test ë°ì´í„° íŒŒì¼ ìœ„ì¹˜**
  * test_data_num[int] -> **test ë°ì´í„° ê°œìˆ˜**
  * test_output_dir[str] -> test ë°ì´í„°ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ **ì¶œë ¥ ê²½ë¡œ**

* Class

  * Layer

    | V or M | name                       | type       | Description                                                  |
    | ------ | -------------------------- | ---------- | ------------------------------------------------------------ |
    | V      | in_dimentsion              | int        | ì…ë ¥ ì°¨ì›                                                    |
    | V      | out_dimension              | int        | ì¶œë ¥ ì°¨ì›                                                    |
    | V      | weight                     | float\[][] | íŒŒë¼ë¯¸í„°                                                     |
    | V      | bias                       | float\[]   | íŒŒë¼ë¯¸í„°                                                     |
    | V      | input                      | float\[][] | forward í•¨ìˆ˜ì—ì„œ ë°›ì€ ì…ë ¥ì„ ì €ì¥(Backwardì—ì„œ ì‚¬ìš©)         |
    | V      | output                     | float\[][] | í•¨ìˆ˜ì—ì„œ ë°›ì€ ì…ë ¥ì„ ì €ì¥(Backwardì—ì„œ ì‚¬ìš©)                 |
    | M      | forward(float\[][])        | float\[][] | ì…ë ¥ì„ ë°›ì•„ ì¶œë ¥ ê°’ì„ return                                 |
    | M      | backward(float\[][],float) | float\[][] | ì…ë ¥ ê°’ì„ í†µí•´ ì–»ì€ ê°’ê³¼ ì´ì „ gradient ê°’ì„ í†µí•´ weightì™€ ì „ë‹¬í•  Gradientë¥¼ ì¶œë ¥í•œë‹¤ |

  * Loss

    | V or M | Name                | Type       | Descroption                                                  |
    | ------ | ------------------- | ---------- | ------------------------------------------------------------ |
    | V      | logit               | float\[][] | ì‹ ê²½ë§ì˜ ìµœì¢… ê²°ê³¼ ê°’ (forwardì—ì„œ ì…ë ¥ ë°›ì•„ ì €ì¥)           |
    | V      | label               | float\[][] | forwardì—ì„œ ì…ë ¥ ë°›ì•„ ì €ì¥                                   |
    | M      | forward(float\[][]) | float\[][] | logitê³¼ labelì„ ì…ë ¥ ë°›ì•„ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¶œë ¥                |
    | M      | backward(void)      | float\[][] | ë‹¤ìŒ layerì— ì „ë‹¬í•  Gradientë¥¼ logitê³¼ labelì„ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ |

  * Dataloader

    | V or M | Name        | Type       | Description                                                  |
    | ------ | ----------- | ---------- | ------------------------------------------------------------ |
    | V      | file_dir    | string     | ë°ì´í„° íŒŒì¼ ê²½ë¡œ                                             |
    | V      | mode        | string     | train, validation,test ëª¨ë“œë¥¼ ì§€ì •                           |
    | V      | feature_num | int        | dataì˜ feature ê°œìˆ˜(ì°¨ì›)                                    |
    | V      | class_num   | int        | ë¶„ë¥˜í•  í´ë˜ìˆ˜ ê°œìˆ˜                                           |
    | V      | data_num    | int        | ë°ì´í„° ê°œìˆ˜                                                  |
    | V      | data        | float\[][] | í•™ìŠµ data                                                    |
    | V      | lable       | float\[][] | test ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ                                |
    | M      | read(void)  | void       | ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ dataì™€ labelì— ê°ê° ì €ì¥í•¨ (test modeì—ì„œëŠ” labelì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ) |

  * Mlp (Multi-Layer Perceptron)

    | V or M | Name                     | Type       | Description                                                |
    | ------ | ------------------------ | ---------- | ---------------------------------------------------------- |
    | V      | layer                    | Layer[]    | Layer ì˜¤ë¸Œì íŠ¸ array-hidden layer ê°œìˆ˜ì— ë”°ë¼ ê¸¸ì´ ë³€ë™    |
    | V      | loss                     | Loss       | Loss ì˜¤ë¸Œì íŠ¸                                              |
    | V      | train_dataset            | Dataloade  | trainì— ì‚¬ìš©í•  Dataloader ì˜¤ë¸Œì íŠ¸                         |
    | V      | train_data               | float\[][] | train_datasetì—ì„œ ì–»ì€ ë°ì´í„°ë¥¼ ì €ì¥                       |
    | V      | train_label              | float\[][] | train_datasetì—ì„œ ì–»ì€ ë ˆì´ë¸” ì €ì¥(one_hot)                |
    | V      | validation_dataset       | Dataloader | validationì— ì‚¬ìš©í•  dataloaderì˜¤ë¸Œì íŠ¸                     |
    | V      | validation_data          | float\[][] | validation_datasetì—ì„œ ì–»ì€ ë°ì´í„°ë¥¼ ì €ì¥                  |
    | V      | validation_label         | float\[][] | validation_datasetì—ì„œ ì–»ì€ ë ˆì´ë¸” ì €ì¥(one_hot)           |
    | V      | epochs                   | int        | epoch                                                      |
    | v      | learning_rate            | float      | learning rate                                              |
    | M      | read_dataset(Dataloader) | void       | ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ dataì™€ labelì— ê°ê° ì €ì¥ í•¨              |
    | M      | add_layer(layer)         | void       | layerì— Layer ì˜¤ë¸Œì íŠ¸ë¥¼ ì¶”ê°€í•¨                            |
    | M      | visulaize_layer(void)    | void       | layer ê³„ì¸µ êµ¬ì¡°ë¥¼ ì¶œë ¥                                     |
    | M      | train(void)              | void       | ê° epochë§ˆë‹¤ lossì™€ ì •í™•ë„ë¥¼ ì¶œë ¥                          |
    | M      | validation(void)         | void       | validation setì— ëŒ€í•´ì„œ lossì™€ ì •í™•ë„ë¥¼ ì¶œë ¥               |
    | M      | predict(Dataloader,str)  | void       | ì…ë ¥í•œ Dataloader(testì„)ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥ |

### Condition

* ê° datasetì— ëŒ€í•´ì„œ í•™ìŠµì´ ì˜ë˜ëŠ” hyper-parameter(epochs,learning-rate, hidden layer ì¸µìˆ˜, neuronì˜ ê°œìˆ˜ ë“±) ë¥¼ ì°¾ëŠ” ì‹¤í—˜ì— ëŒ€í•œ ê³¼ì • ë° ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì œì¶œí•´ì•¼ í•¨. í•„ìˆ˜ ì‹¤í—˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤

| Epochs | Learning Rate ($$\eta$$) | Hidden Layer ê°œìˆ˜ | Neuron ê°œìˆ˜(Perceptron) |
| ------ | ------------------------ | ----------------- | ----------------------- |
| 100    | 0.01                     | 1                 | 10                      |
| 500    | 0.05                     | 2                 | 10,10                   |
| 1000   | 0.01                     | 3                 | 30,100,30               |
| 1000   | 0.03                     | 2                 | 100,100                 |

* ìœ„ ì¡°ê±´ì— ë”°ë¼ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ê¸°ì…í•˜ê³  ìì‹ ì´ ì°¾ì€ ìµœì ì˜ ê°’ì„ ì°¾ëŠ” ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì‘ì„±í•´ í•¨
* í•™ìŠµí•œ ë„¤íŠ¸ì›Œí¬ë¡œ ê° datasetì— ì¡´ì¬í•˜ëŠ” test dataset(ì •ë‹µ ë ˆì´ë¸”ì´ ì—†ëŠ”)ì— ëŒ€í•´ [Test ê²°ê³¼ ì˜ˆì‹œ]ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ê³¼ì œ ì œì¶œ ì‹œ ì†ŒìŠ¤ì½”ë“œì™€ í•¨ê¼ ì œì¶œí•´ì•¼ í•¨

---

### Ref

* [LOVITxDATA SCIENCE]([https://lovit.github.io/machine%20learning/2018/04/27/synthetic_dataset/](https://lovit.github.io/machine learning/2018/04/27/synthetic_dataset/))
  * soydaya, ë³µì¡í•œ ì¸ê³µ ë°ì´í„° ìƒì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤.....**Skit-Learn**
* [í¸ë¯¸ë¶„ê³¼ Gradientì˜ ê¸°í•˜í•™ì  ì˜ë¯¸](https://www.youtube.com/watch?v=bUNqn1G1O7E)
* [Khan: ChainRule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction)
* [Matrixì˜ ê¸°ë³¸ ì§€ì‹](http://researchhubs.com/post/maths/fundamentals/basic-of-matrix.html)
* [í•˜ë‹¤ë§ˆë“œ ê³±: $$\odot$$](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
* [Matrixì˜ ê³±]([https://ko.wikipedia.org/wiki/%ED%96%89%EB%A0%AC_%EA%B3%B1%EC%85%88](https://ko.wikipedia.org/wiki/í–‰ë ¬_ê³±ì…ˆ))
* [1-07. Multi Layer Perceptron ì´ì •ë¦¬](